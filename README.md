# Health Policy Mapper

This repository contains the Health Policy Mapper system, an automated tool for collecting and extracting data from official public health documents using Large Language Models (LLMs). Currently, only the extraction module has been developed.

---

## Extraction Module
Through the upload of public health documents, along with research context and the definition of expected columns (with descriptions provided by the user), the system performs structured information extraction, generates justifications for each identified variable, and consolidates the results into tables ready for analysis. This reduces manual effort for researchers and enables more agile and consistent monitoring of health policies.

## Folder Structure

```
├── front-end/
├── src/
├── data/
├── .env
├── .gitignore
├── README.md
├── pyproject.toml
├── poetry.lock
├── poetry.toml
```

Descriptions
- front-end/: web application (frontend interface) of the project.
- src/: FastAPI backend with use cases, entities and integrations.
- data/: reference files and outputs generated by evaluations.
- .gitignore
- README.md
- pyproject.toml
- poetry.lock
- poetry.toml: Poetry configuration files.

---

## How to Run (Prerequisites and Steps)

Prerequisites
- Python 3.10+ (backend)
- Node.js 18+ (for frontend)
- Poetry installed
- Google Generative AI API key (GOOGLE_API_KEY)

Steps
1) Install dependencies
```bash
poetry install
```

2) Configure environment variables (create a `.env` file in the root)
```env
GOOGLE_API_KEY=your_google_genai_key
```

3) Start the server (hot reload)
```bash
poetry run fastapi dev src/main.py
```

4) Access the documentation
- Swagger UI: http://localhost:8000/docs

### Frontend

Run
```bash
cd front-end
npm install
npm run dev
```

Default port
- http://localhost:3000

---

## Workflow

- Job Creation: `POST /jobs/` with multipart form containing `files` (PDFs), `context` (string) and `columns` (JSON `[ {"name","description"} ]`). Returns `job_id` and processes in background.
- Processing: `LLMProcessor` calls `GeminiClient` per file, generates incremental raw rows (`raw_data_<job>.csv`), and updates progress and errors. At the end, `Aggregator` consolidates by country and Job is marked as `done` or `done_with_errors`.
- Monitoring: `GET /jobs/{job_id}/status` returns status, progress and error count; `GET /jobs/{job_id}/raw` returns incremental CSV or JSON; `GET /jobs/{job_id}/result` downloads the final aggregated CSV.
- Recovery: `POST /jobs/{job_id}/retry-failed-records` removes error rows from raw CSV, reinitializes job for retry and returns clean CSV; `POST /jobs/{job_id}/resume` continues remaining processing.
- Evaluation: `POST /eval/` with `file` (aggregated CSV) and `context=90_prep_sti` compares with reference dataset and saves metrics + CSV with highlighted errors in `data/output/90_prep_sti/<model_date>/`.

---
